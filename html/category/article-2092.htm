<!DOCTYPE html>
<html>

<head>
        <link rel="canonical" href="https://laosaddress.github.io/html/category/article-2092.htm" />
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>深度学习自定义自动求导函数 - Laos Address</title>
        <link rel="icon" href="/assets/addons/xcblog/img/laosaddress/favicon.ico" type="image/x-icon"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <!-- Bootstrap CSS-->
    <link rel="stylesheet" href="/assets/addons/xcblog/js/frontend/laosaddress/bootstrap/css/bootstrap.min.css">
    <!-- Font Awesome CSS-->
    <link rel="stylesheet" href="/assets/addons/xcblog/js/frontend/laosaddress/font-awesome/css/font-awesome.min.css">
    <!-- Google fonts - Poppins-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Poppins:300,400,600">
    <!-- Lightbox-->
    <link rel="stylesheet" href="/assets/addons/xcblog/js/frontend/laosaddress/lightbox2/css/lightbox.css">
    <link rel="stylesheet" href="/assets/addons/xcblog/css/laosaddress/fontastic.css">
    <!-- theme stylesheet-->
    <link rel="stylesheet" href="/assets/addons/xcblog/css/laosaddress/style.default.css" id="theme-stylesheet">
    <!-- Custom stylesheet - for your changes-->
    <link rel="stylesheet" href="/assets/addons/xcblog/css/laosaddress/custom.css">
    <!-- Tweaks for older IEs-->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script><![endif]-->
    <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?a08ccaf1644b67c8d98a3563104919a8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body>
        <!-- navbar-->
    <header class="header">
        <nav class="navbar navbar-expand-lg fixed-top">
            <div class="container">
                                <a href="/" class="navbar-brand">
                    Laos Address
                </a>
                
                <button type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler navbar-toggler-right">Menu<i class="fa fa-bars ml-2"></i></button>
                <div id="navbarSupportedContent" class="collapse navbar-collapse">
                    <ul class="navbar-nav ml-auto">
                                                <!-- Link-->
                        <li class="nav-item"> <a href="/" class="nav-link">首页</a></li>
                                                <!-- Link-->
                        <li class="nav-item"> <a href="/html/category/" class="nav-link">文章分类</a></li>
                                                <!-- Link-->
                        <li class="nav-item"> <a href="#" class="nav-link">关于</a></li>
                        <!-- Link-->
                        <li class="nav-item"> <a href="#" class="nav-link">联系</a></li>
                    </ul>
                </div>
            </div>
        </nav>
    </header>

    <!-- Hero Section-->
    <section class="hero">
        <div class="container text-center">
            <h1>深度学习自定义自动求导函数</h1>
            <nav aria-label="breadcrumb" class="d-flex justify-content-center">
                <ol class="breadcrumb">
                    <li class="breadcrumb-item"><a href="/" class="animsition-link">首页</a></li>
                    <li class="breadcrumb-item"><a href="/html/category/" class="animsition-link">文章分类</a></li>
                    <li aria-current="page" class="breadcrumb-item active">正文</li>
                </ol>
            </nav>
        </div>
    </section>
    <section>
        <div class="container">
            <div class="row" id="list">
                <div class="col-md-9">
                          				  				  				<div id="content_views" class="markdown_views prism-atom-one-dark"> <p>参考：链接1，链接2</p> <p>官方示例：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable   <span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""     We can implement our own custom autograd Functions by subclassing     torch.autograd.Function and implementing the forward and backward passes     which operate on Tensors.     """</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the forward pass we receive a Tensor containing the input and return         a Tensor containing the output. ctx is a context object that can be used         to stash information for backward computation. You can cache arbitrary         objects for use in the backward pass using the ctx.save_for_backward method.          """</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span> <span class="token comment"># ctx 用来保存反向求导所需要的数据,也就是可以在backward（）函数中使用的变量。</span>         <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the backward pass we receive a Tensor containing the gradient of the loss         with respect to the output, and we need to compute the gradient of the loss         with respect to the input.         """</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>         grad_input<span class="token punctuation">[</span><span class="token builtin">input</span> <span class="token operator"><</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>         <span class="token keyword">return</span> grad_input <span class="token comment">#反向传播求梯度，如果该参数为网络需要更新的参数，那么该梯度会被保存，方便之后的参数更新或者优化。</span>   dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor <span class="token comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span>  <span class="token comment"># N is batch size; D_in is input dimension;</span> <span class="token comment"># H is hidden dimension; D_out is output dimension.</span> N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>  <span class="token comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span> x <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> y <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># Create random Tensors for weights, and wrap them in Variables.</span> w1 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> w2 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  learning_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span>     relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span>      <span class="token comment"># Forward pass: compute predicted y using operations on Variables; we compute</span>     <span class="token comment"># ReLU using our custom autograd operation.</span>     y_pred <span class="token operator">=</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>      <span class="token comment"># Compute and print loss</span>     loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment"># Use autograd to compute the backward pass.</span>     loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播会将可训练参数梯度保存。</span>      <span class="token comment"># Update weights using gradient descent</span>     w1<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data     w2<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data      <span class="token comment"># Manually zero the gradients after updating weights</span>     w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#梯度清零。</span>     w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>  </code></pre> <p>类继承：<br /><code>torch.autograd.Function</code>，只需要定义正向传播和反向传播函数，正向传播就是自己定义函数的计算方法；反向传播则是求导梯度，<code>ctx</code>这个东西就当做<code>self</code>来对待就行，可以用来存储反向求导要求的数据，比如正向传播的结果或者输入。</p> <h3> 自定义Linear 操作：</h3> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Function <span class="token keyword">import</span> warnings warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>    <span class="token keyword">class</span> <span class="token class-name">LinearFunction1</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">""" 描述：在pytorch中自定义一个操作，并定义它的梯度求法"""</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>   <span class="token comment"># shape: n,m,  m nout</span>         <span class="token comment"># ctx.needs_input_grad = (False,True,True)</span>         output <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">)</span>  <span class="token comment"># n,m; m,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>             <span class="token comment"># output += torch.unsqueeze(bias,dim=0).expand_as(output)</span>             <span class="token comment"># output += bias   #广播。</span>         <span class="token comment"># ctx.save_for_backward(output)</span>         <span class="token keyword">return</span> output      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> <span class="token boolean">None</span>         grad_weight <span class="token operator">=</span> <span class="token boolean">None</span>         grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_outputs @ <span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># n,c_out;c_out,m</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> @ grad_outputs  <span class="token comment"># m,n    n,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>              grad_bias <span class="token operator">=</span> grad_outputs<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span>grad_weight<span class="token punctuation">,</span>grad_bias  <span class="token comment"># Inherit from Function</span> <span class="token keyword">class</span> <span class="token class-name">LinearFunction</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment"># Note that both forward and backward are @staticmethods</span>     @<span class="token builtin">staticmethod</span>     <span class="token comment"># bias is an optional argument</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>         output <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 20,20; 30,20 -> 20,30</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>         <span class="token keyword">return</span> output      <span class="token comment"># This function has only a single output, so it gets only one gradient</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># This is a pattern that is very convenient - at the top of backward</span>         <span class="token comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>         <span class="token comment"># None. Thanks to the fact that additional trailing Nones are</span>         <span class="token comment"># ignored, the return statement is simple even when the function has</span>         <span class="token comment"># optional inputs.</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_weight <span class="token operator">=</span> grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>          <span class="token comment"># These needs_input_grad checks are optional and there only to</span>         <span class="token comment"># improve efficiency. If you want to make your code simpler, you can</span>         <span class="token comment"># skip them. Returning gradients for inputs that don't require it is</span>         <span class="token comment"># not an error.</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>   <span class="token comment"># 20 30 , 30 20  -> 20 20   或者 20 30 30 20</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>  <span class="token comment"># 30 20, 20 20 - > 30 20</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_bias <span class="token operator">=</span> grad_output<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span> grad_weight<span class="token punctuation">,</span> grad_bias </code></pre> <p>也就是定义正向传播和反向传播并保存需要的数据到context中即可，函数内的数学运算可以不是pytorch支持的运算而只需要是python支持支持的即可（个人理解是这样）。</p> <p>测试操作是否正确：</p> <pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> gradcheck  linear <span class="token operator">=</span> LinearFunction<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span>   linear <span class="token operator">=</span> LinearFunction1<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span> </code></pre> <p>可以看到两种方都返回的是<code>True</code>,需要注意的是自己定义的操作输入的形状等问题而已。</p> <h4> 用自己的操作来建立模型：</h4> <pre><code class="prism language-cpp">import torch<span class="token punctuation">.</span>nn as nn <span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>     def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">,</span> bias<span class="token operator">=</span>True<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token function">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>input_features <span class="token operator">=</span> input_features         self<span class="token punctuation">.</span>output_features <span class="token operator">=</span> output_features          self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">else</span><span class="token operator">:</span>             self<span class="token punctuation">.</span><span class="token function">register_parameter</span><span class="token punctuation">(</span><span class="token string">"bias"</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>          <span class="token macro property"># self.weight.uniform(-0.1, 0.1)</span>         nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>      def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token keyword">return</span> <span class="token function">LinearFunction1</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span> # 调用自定义的操作。   </code></pre> <p>在自定义操作的基础上建立的<code>layer</code>就与其他<code>layer</code>一样都可以自动求导和优化参数了。</p> <h4> 不可导情况：</h4> <p>上边的线性变换是的运算是可导的情况，也就是可以从输出一步步的用导数或者运算来表达，如果遇到那种不可导的情况，也就是无法显式的表达导数该怎么办？那就是自己制定导数求法，比如近似求导或者干脆用另一个黑盒函数来进行代替求导。那么既然是黑盒函数，那么反向的传播的时候函数中间的值的梯度什么的就很难进行计算了，这种问题就需要对<code>backward</code>函数进行稍微的改变：</p> <pre><code class="prism language-python"><span class="token triple-quoted-string string">"""当某个操作是不可导的，但是你却用了近似的方法来代替。"""</span> <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>function <span class="token keyword">import</span> once_differentiable  <span class="token keyword">def</span> <span class="token function">un_differentibale_function</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token string">"一些列不可导的神奇操作"</span>      grad_output_changed <span class="token operator">=</span> <span class="token boolean">None</span>      <span class="token keyword">return</span>  grad_output_changed   @<span class="token builtin">staticmethod</span> @once_differentiable <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">)</span>     grad_output_changed <span class="token operator">=</span> un_differentibale_function<span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span>     grad_input <span class="token operator">=</span> grad_output_changed     <span class="token keyword">return</span> grad_input </code></pre> <p><code>@once_differentiable</code> 神马意思，我也说不清，就当做隐式求导或者近似求导吧。</p> </p></div> 			                        <div class="col-md-12 mt-5">
                                                        <p>上一个：<a href="/html/category/article-2091.htm">df.fillna()函数，参数method取值ffill</a></p>
                                                        <p>下一个：<a href="/html/category/article-2093.htm">VUE项目解决Vant组件库样式修改不生效问题</a></p>
                                                    </div>
                                        </div>
                <div class="col-md-3">
                    <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/html/category/article-3135.htm" title="Spring 源码BeanFactoryPostProcessor是如何执行的">Spring 源码BeanFactoryPostProcessor是如何执行的</a></li>
                        <li class="py-2"><a href="/html/category/article-4417.htm" title="开宠物食品店需要多少钱 开宠物食品店需要多少钱一个月">开宠物食品店需要多少钱 开宠物食品店需要多少钱一个月</a></li>
                        <li class="py-2"><a href="/html/category/article-6013.htm" title="宠爱国际动物医院电话北京（北京宠爱国际动物医疗）">宠爱国际动物医院电话北京（北京宠爱国际动物医疗）</a></li>
                        <li class="py-2"><a href="/html/category/article-4138.htm" title="在VS2015下配置websocket++,并用C++搭建一个简单的客户端">在VS2015下配置websocket++,并用C++搭建一个简单的客户端</a></li>
                        <li class="py-2"><a href="/html/category/article-4306.htm" title="欧阳娜娜翟子路 图片（不要告诉别人）欧阳娜娜翟子路微博，轩辕剑如烟，邱非，">欧阳娜娜翟子路 图片（不要告诉别人）欧阳娜娜翟子路微博，轩辕剑如烟，邱非，</a></li>
                        <li class="py-2"><a href="/html/category/article-4416.htm" title="办理动物诊疗许可证需要什么材料洛阳（动物诊疗许可证需要多少钱）">办理动物诊疗许可证需要什么材料洛阳（动物诊疗许可证需要多少钱）</a></li>
                        <li class="py-2"><a href="/html/category/article-2234.htm" title="CI框架源码解析十九之分页类文件Pagination.php">CI框架源码解析十九之分页类文件Pagination.php</a></li>
                        <li class="py-2"><a href="/html/category/article-4383.htm" title="动物注射疫苗属于什么免疫方式（动物打疫苗获得的免疫称为）">动物注射疫苗属于什么免疫方式（动物打疫苗获得的免疫称为）</a></li>
                        <li class="py-2"><a href="/html/category/article-4529.htm" title="瑞鹏宠物医院在线咨询免费（瑞鹏宠物医院客服电话）">瑞鹏宠物医院在线咨询免费（瑞鹏宠物医院客服电话）</a></li>
                        <li class="py-2"><a href="/html/category/article-3770.htm" title="Java 使用 Maven BOM 统一管理版本号_在线工具">Java 使用 Maven BOM 统一管理版本号_在线工具</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">38</span> <a href="/html/date/2024-07/" title="2024-07 归档">2024-07</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">60</span> <a href="/html/date/2024-06/" title="2024-06 归档">2024-06</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">62</span> <a href="/html/date/2024-05/" title="2024-05 归档">2024-05</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">60</span> <a href="/html/date/2024-04/" title="2024-04 归档">2024-04</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">62</span> <a href="/html/date/2024-03/" title="2024-03 归档">2024-03</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">44</span> <a href="/html/date/2024-02/" title="2024-02 归档">2024-02</a></h4>
            </li>
                    </ul>
    </div>
</div>



                </div>
            </div>
        </div>
    </section>
        <footer class="main-footer">
        <div class="copyrights">
            <div class="container">
                <div class="row">
                    <div class="col-lg-6 text-center text-lg-left">
                        <p class="copyrights-text mb-3 mb-lg-0">
                          Laos Address 版权所有 Powered by WordPress
                        </p>
                    </div>
                    <div class="col-lg-6 text-center text-lg-right">
                        <ul class="list-inline social mb-0">
                            <li class="list-inline-item"><a href="#" class="social-link"><i class="fa fa-facebook"></i></a><a href="#" class="social-link"><i class="fa fa-twitter"></i></a><a href="#" class="social-link"><i class="fa fa-youtube-play"></i></a><a href="#" class="social-link"><i class="fa fa-vimeo"></i></a><a href="#" class="social-link"><i class="fa fa-pinterest"></i></a></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- JavaScript files-->
    <script src="/assets/addons/xcblog/js/frontend/laosaddress/jquery/jquery.min.js"></script>
    <script src="/assets/addons/xcblog/js/frontend/laosaddress/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="/assets/addons/xcblog/js/frontend/laosaddress/jquery.cookie/jquery.cookie.js"> </script>
    <script src="/assets/addons/xcblog/js/frontend/laosaddress/lightbox2/js/lightbox.js"></script>
    <script src="/assets/addons/xcblog/js/frontend/laosaddress/front.js"></script>
</body>

</html>